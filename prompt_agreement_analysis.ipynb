{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Agreement Across Prompt Types by Topic\n",
    "\n",
    "This notebook analyzes how often the classifier's labels (bias categories) are in agreement across the three prompt types (Generic, Young Adult, Senior Adult) for each topic in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Increase max column width for better text display\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the bias_category_responses.csv file\n",
    "file_path = 'model_bias_score/bias_category_responses.csv'\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract prompt types based on patterns in the prompt column\n",
    "def extract_prompt_type(prompt):\n",
    "    if 'Imagine you are a 25 year old US citizen' in prompt:\n",
    "        return 'Young Adult (25 years old)'\n",
    "    elif 'Imagine you are a 60 year old US citizen' in prompt:\n",
    "        return 'Senior Adult (60 years old)'\n",
    "    else:\n",
    "        return 'Generic'\n",
    "\n",
    "# Apply the function to create a new column for prompt type\n",
    "df['prompt_type'] = df['prompt'].apply(extract_prompt_type)\n",
    "\n",
    "# Display the distribution of prompt types\n",
    "print(\"Distribution of prompt types:\")\n",
    "prompt_type_counts = df['prompt_type'].value_counts()\n",
    "print(prompt_type_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Label Agreement Across Prompt Types by Topic\n",
    "\n",
    "Let's analyze how often the classifier's labels (bias categories) are in agreement across the three prompt types for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new section to analyze agreement across prompt types for each topic\n",
    "print(\"\\nAnalyzing agreement of bias categories across prompt types for each topic:\")\n",
    "\n",
    "# Initialize counters for different agreement scenarios\n",
    "agreement_counts = {}\n",
    "\n",
    "# Create a dataframe to store detailed agreement information by topic\n",
    "topic_agreement_details = []\n",
    "\n",
    "# Iterate through each topic\n",
    "for topic in df['topic'].unique():\n",
    "    # Filter data for the current topic\n",
    "    topic_data = df[df['topic'] == topic]\n",
    "    \n",
    "    # Group by model and prompt_type to get the bias_category for each combination\n",
    "    topic_models = topic_data['model'].unique()\n",
    "    \n",
    "    # For each model, analyze agreement across prompt types\n",
    "    for model in topic_models:\n",
    "        model_topic_data = topic_data[topic_data['model'] == model]\n",
    "        \n",
    "        # Group by prompt_type and get the bias_category for each\n",
    "        prompt_bias_dict = {}\n",
    "        for prompt_type in df['prompt_type'].unique():\n",
    "            prompt_data = model_topic_data[model_topic_data['prompt_type'] == prompt_type]\n",
    "            if len(prompt_data) > 0:\n",
    "                # Get the bias category for this prompt type, model and topic\n",
    "                bias_category = prompt_data['bias_category'].values[0]\n",
    "                prompt_bias_dict[prompt_type] = bias_category\n",
    "        \n",
    "        # Count unique bias categories to determine level of agreement\n",
    "        unique_categories = set(prompt_bias_dict.values())\n",
    "        num_unique = len(unique_categories)\n",
    "        \n",
    "        # Determine agreement type\n",
    "        if num_unique == 1:\n",
    "            agreement_type = \"full_agreement\"  # All 3 prompt types have the same bias category\n",
    "        elif num_unique == 2:\n",
    "            agreement_type = \"partial_agreement\"  # 2 prompt types have the same bias category\n",
    "        else:\n",
    "            agreement_type = \"no_agreement\"  # All 3 prompt types have different bias categories\n",
    "        \n",
    "        # Update agreement counts\n",
    "        agreement_counts[agreement_type] = agreement_counts.get(agreement_type, 0) + 1\n",
    "        \n",
    "        # Store detailed information for this topic and model\n",
    "        topic_agreement_details.append({\n",
    "            'topic': topic,\n",
    "            'model': model,\n",
    "            'agreement_type': agreement_type,\n",
    "            'unique_categories': num_unique,\n",
    "            'categories': list(unique_categories),\n",
    "            'prompt_type_categories': prompt_bias_dict\n",
    "        })\n",
    "\n",
    "# Convert to dataframe for easier analysis\n",
    "topic_agreement_df = pd.DataFrame(topic_agreement_details)\n",
    "\n",
    "# Display agreement counts\n",
    "print(\"\\nAgreement counts across prompt types:\")\n",
    "for agreement_type, count in agreement_counts.items():\n",
    "    print(f\"{agreement_type}: {count} topic-model combinations\")\n",
    "\n",
    "# Calculate percentage of topic-model combinations with each agreement type\n",
    "total_combinations = len(topic_agreement_df)\n",
    "print(\"\\nPercentage of topic-model combinations with each agreement type:\")\n",
    "for agreement_type, count in agreement_counts.items():\n",
    "    percentage = (count / total_combinations) * 100\n",
    "    print(f\"{agreement_type}: {percentage:.2f}%\")\n",
    "\n",
    "# Display the first few rows of the detailed agreement dataframe\n",
    "print(\"\\nDetailed agreement information by topic and model (first few rows):\")\n",
    "topic_agreement_df[['topic', 'model', 'agreement_type', 'unique_categories', 'categories']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the agreement counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(agreement_counts.keys()), y=list(agreement_counts.values()))\n",
    "plt.title('Agreement of Bias Categories Across Prompt Types')\n",
    "plt.xlabel('Agreement Type')\n",
    "plt.ylabel('Number of Topic-Model Combinations')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze agreement by topic (across all models)\n",
    "print(\"\\nAnalyzing agreement by topic (across all models):\")\n",
    "\n",
    "# Group by topic and count agreement types\n",
    "topic_agreement_counts = topic_agreement_df.groupby('topic')['agreement_type'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Calculate the total number of models for each topic\n",
    "topic_model_counts = topic_agreement_df.groupby('topic')['model'].nunique()\n",
    "\n",
    "# Calculate percentage of models with each agreement type for each topic\n",
    "topic_agreement_percentages = topic_agreement_counts.div(topic_model_counts, axis=0) * 100\n",
    "\n",
    "# Round to 2 decimal places\n",
    "topic_agreement_percentages = topic_agreement_percentages.round(2)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nPercentage of models with each agreement type by topic:\")\n",
    "topic_agreement_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize agreement by topic\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Create a stacked bar chart\n",
    "topic_agreement_percentages.plot(kind='bar', stacked=True, figsize=(14, 8))\n",
    "plt.title('Agreement of Bias Categories Across Prompt Types by Topic')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Percentage of Models')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Agreement Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze agreement by model (across all topics)\n",
    "print(\"\\nAnalyzing agreement by model (across all topics):\")\n",
    "\n",
    "# Group by model and count agreement types\n",
    "model_agreement_counts = topic_agreement_df.groupby('model')['agreement_type'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Calculate the total number of topics for each model\n",
    "model_topic_counts = topic_agreement_df.groupby('model')['topic'].nunique()\n",
    "\n",
    "# Calculate percentage of topics with each agreement type for each model\n",
    "model_agreement_percentages = model_agreement_counts.div(model_topic_counts, axis=0) * 100\n",
    "\n",
    "# Round to 2 decimal places\n",
    "model_agreement_percentages = model_agreement_percentages.round(2)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nPercentage of topics with each agreement type by model:\")\n",
    "model_agreement_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize agreement by model\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a stacked bar chart\n",
    "model_agreement_percentages.plot(kind='bar', stacked=True, figsize=(12, 8))\n",
    "plt.title('Agreement of Bias Categories Across Prompt Types by Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Percentage of Topics')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Agreement Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis has examined how often the classifier's labels (bias categories) are in agreement across the three prompt types for each topic. The results provide insights into the consistency of political bias classification across different prompt formulations, which could be useful for understanding how prompt engineering affects bias detection in language model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a markdown cell for the new analysis section
    print(\"\\nAnalyzing agreement by topic grouped by model:\")\n",
    "
    # Create a cross-tabulation of agreement types for each topic-model combination
    topic_model_agreement = pd.crosstab(\n",
    "    [topic_agreement_df['topic'], topic_agreement_df['model']], 
    "    topic_agreement_df['agreement_type'],
    "    normalize='index',  # Normalize by row (topic-model combination)
    "    margins=True,       # Include row and column totals
    "    margins_name='Total'
    ") * 100  # Convert to percentage
    
    # Round to 2 decimal places
    topic_model_agreement = topic_model_agreement.round(2)
    
    # Display the results
    print(\"\\nPercentage of agreement types for each topic-model combination:\")
    topic_model_agreement
    
    # Create a heatmap visualization of the agreement types for each topic-model combination
    # First, reshape the data to have topics as rows, models as columns, and cell values as agreement percentages
    
    # Filter out the 'Total' row and column for better visualization
    topic_model_agreement_filtered = topic_model_agreement.drop('Total').drop(columns='Total')
    
    # Create a pivot table with topics as rows, models as columns, and full_agreement percentage as values
    pivot_full_agreement = topic_model_agreement_filtered.reset_index().pivot(\n",
    "    index='topic', 
    "    columns='model', 
    "    values='full_agreement'
    ")
    
    # Create the heatmap
    plt.figure(figsize=(14, 10))
    sns.heatmap(pivot_full_agreement, annot=True, cmap='YlGnBu', fmt='.1f', linewidths=.5)
    plt.title('Percentage of Full Agreement Across Prompt Types by Topic and Model')
    plt.ylabel('Topic')
    plt.xlabel('Model')
    plt.tight_layout()
    plt.show()
    
    # Create a grouped bar chart to compare agreement types across topics for each model
    # Reshape the data for easier plotting
    agreement_by_topic_model = topic_agreement_df.groupby(['topic', 'model'])['agreement_type'].value_counts(normalize=True).unstack(fill_value=0) * 100
    
    # Round to 2 decimal places
    agreement_by_topic_model = agreement_by_topic_model.round(2)
    
    # Get unique models for plotting
    models = df['model'].unique()
    
    # Create a figure with subplots for each model
    fig, axes = plt.subplots(len(models), 1, figsize=(14, 5*len(models)), sharex=True)
    
    # If there's only one model, axes won't be an array, so convert it to one
    if len(models) == 1:
        axes = [axes]
    
    # Plot data for each model
    for i, model in enumerate(models):
        # Filter data for this model
        model_data = agreement_by_topic_model.xs(model, level=1, drop_level=False).droplevel(1)
        
        # Plot the data
        model_data.plot(kind='bar', stacked=True, ax=axes[i])
        axes[i].set_title(f'Agreement Types by Topic for {model}')
        axes[i].set_ylabel('Percentage')
        axes[i].set_ylim(0, 100)
        axes[i].legend(title='Agreement Type')
    
    # Set common x-label
    plt.xlabel('Topic')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "political-bias-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
