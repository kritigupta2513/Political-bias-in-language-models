{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Political Bias Categories by Prompt Type and Model\n",
    "\n",
    "This notebook analyzes the `bias_category_responses.csv` file to group responses by prompt type and model, then examines the distribution of bias categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Increase max column width for better text display\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the bias_category_responses.csv file\n",
    "file_path = 'model_bias_score/bias_category_responses.csv'\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "# print(f\"Dataset shape: {df.shape}\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the columns in the dataset\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check unique values in key columns\n",
    "print(\"Unique models:\")\n",
    "print(df['model'].unique())\n",
    "\n",
    "print(\"Unique topics:\")\n",
    "print(df['topic'].unique())\n",
    "\n",
    "print(\"Unique bias categories:\")\n",
    "print(df['bias_category'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Prompt Types\n",
    "\n",
    "Let's extract the different prompt types from the 'prompt' column to group by them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract prompt types based on patterns in the prompt column\n",
    "def extract_prompt_type(prompt):\n",
    "    if 'Imagine you are a 25 year old US citizen' in prompt:\n",
    "        return 'Young Adult (25 years old)'\n",
    "    elif 'Imagine you are a 60 year old US citizen' in prompt:\n",
    "        return 'Senior Adult (60 years old)'\n",
    "    else:\n",
    "        return 'Generic'\n",
    "\n",
    "# Apply the function to create a new column for prompt type\n",
    "df['prompt_type'] = df['prompt'].apply(extract_prompt_type)\n",
    "\n",
    "# Display the distribution of prompt types\n",
    "print(\"Distribution of prompt types:\")\n",
    "prompt_type_counts = df['prompt_type'].value_counts()\n",
    "print(prompt_type_counts)\n",
    "\n",
    "# Visualize the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='prompt_type')\n",
    "plt.title('Distribution of Prompt Types')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by Model and Prompt Type\n",
    "\n",
    "Now, let's analyze the distribution of bias categories grouped by model and prompt type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model and prompt_type, and count bias categories\n",
    "bias_by_model_prompt = df.groupby(['model', 'prompt_type'])['bias_category'].value_counts(normalize=True).unstack(fill_value=0) * 100\n",
    "\n",
    "# Round to 2 decimal places for better readability\n",
    "bias_by_model_prompt = bias_by_model_prompt.round(2)\n",
    "\n",
    "# Display the results\n",
    "print(\"Bias category distribution by model and prompt type (percentages):\")\n",
    "bias_by_model_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution with a heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Reset index to prepare for pivot\n",
    "bias_heatmap_data = bias_by_model_prompt.reset_index()\n",
    "\n",
    "# Create pivot table for heatmap\n",
    "# If there are multiple bias categories (left, center, right)\n",
    "if len(df['bias_category'].unique()) > 1:\n",
    "    for category in sorted(df['bias_category'].unique()):\n",
    "        # Skip if the category column doesn't exist in the dataframe\n",
    "        if category not in bias_heatmap_data.columns:\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pivot_data = bias_heatmap_data.pivot(index='model', columns='prompt_type', values=category)\n",
    "        sns.heatmap(pivot_data, annot=True, cmap='YlGnBu', fmt='.1f', cbar_kws={'label': f'Percentage of {category} bias'})\n",
    "        plt.title(f'Percentage of {category.capitalize()} Bias by Model and Prompt Type')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    # If there's only one bias category\n",
    "    category = df['bias_category'].unique()[0]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pivot_data = bias_heatmap_data.pivot(index='model', columns='prompt_type', values=category)\n",
    "    sns.heatmap(pivot_data, annot=True, cmap='YlGnBu', fmt='.1f', cbar_kws={'label': f'Percentage of {category} bias'})\n",
    "    plt.title(f'Percentage of {category.capitalize()} Bias by Model and Prompt Type')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis by Topic and Prompt Type\n",
    "\n",
    "Let's also analyze how bias categories are distributed across different topics and prompt types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by topic and prompt_type, and count bias categories\n",
    "bias_by_topic_prompt = df.groupby(['topic', 'prompt_type'])['bias_category'].value_counts(normalize=True).unstack(fill_value=0) * 100\n",
    "\n",
    "# Round to 2 decimal places for better readability\n",
    "bias_by_topic_prompt = bias_by_topic_prompt.round(2)\n",
    "\n",
    "# Display the results\n",
    "print(\"Bias category distribution by topic and prompt type (percentages):\")\n",
    "bias_by_topic_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart to compare bias categories across models for each prompt type\n",
    "# First, calculate the counts (not percentages) for better visualization\n",
    "bias_counts = df.groupby(['model', 'prompt_type', 'bias_category']).size().reset_index(name='count')\n",
    "\n",
    "# Plot for each prompt type\n",
    "for prompt_type in df['prompt_type'].unique():\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Filter data for the current prompt type\n",
    "    prompt_data = bias_counts[bias_counts['prompt_type'] == prompt_type]\n",
    "    \n",
    "    # Create the grouped bar chart\n",
    "    sns.barplot(data=prompt_data, x='model', y='count', hue='bias_category')\n",
    "    \n",
    "    plt.title(f'Bias Category Distribution by Model for {prompt_type} Prompts')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Bias Category')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Score Analysis\n",
    "\n",
    "Let's analyze the confidence scores for different bias categories across models and prompt types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average confidence score by model, prompt type, and bias category\n",
    "avg_confidence = df.groupby(['model', 'prompt_type', 'bias_category'])['confidence_score'].mean().reset_index()\n",
    "\n",
    "# Round to 4 decimal places\n",
    "avg_confidence['confidence_score'] = avg_confidence['confidence_score'].round(4)\n",
    "\n",
    "# Display the results\n",
    "print(\"Average confidence score by model, prompt type, and bias category:\")\n",
    "avg_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize average confidence scores\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create the grouped bar chart\n",
    "sns.barplot(data=avg_confidence, x='model', y='confidence_score', hue='bias_category')\n",
    "\n",
    "plt.title('Average Confidence Score by Model and Bias Category')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Average Confidence Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Bias Category')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Let's create a summary table with key statistics for each model and prompt type combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary dataframe\n",
    "summary = []\n",
    "\n",
    "for model in df['model'].unique():\n",
    "    for prompt_type in df['prompt_type'].unique():\n",
    "        # Filter data for current model and prompt type\n",
    "        subset = df[(df['model'] == model) & (df['prompt_type'] == prompt_type)]\n",
    "        \n",
    "        if len(subset) > 0:  # Only process if there's data for this combination\n",
    "            # Calculate statistics\n",
    "            bias_counts = subset['bias_category'].value_counts().to_dict()\n",
    "            total = len(subset)\n",
    "            avg_confidence = subset['confidence_score'].mean()\n",
    "            \n",
    "            # Create a row for the summary table\n",
    "            row = {\n",
    "                'model': model,\n",
    "                'prompt_type': prompt_type,\n",
    "                'total_responses': total,\n",
    "                'avg_confidence': round(avg_confidence, 4)\n",
    "            }\n",
    "            \n",
    "            # Add bias category percentages\n",
    "            for category in sorted(df['bias_category'].unique()):\n",
    "                count = bias_counts.get(category, 0)\n",
    "                percentage = (count / total) * 100 if total > 0 else 0\n",
    "                row[f'{category}_count'] = count\n",
    "                row[f'{category}_percentage'] = round(percentage, 2)\n",
    "            \n",
    "            summary.append(row)\n",
    "\n",
    "# Convert to dataframe\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Display the summary table\n",
    "print(\"Summary statistics by model and prompt type:\")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This analysis has examined how different language models exhibit political bias across various prompt types. The results show patterns in how models respond to different prompt formulations, which could be useful for understanding and mitigating bias in language model outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "political-bias-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
